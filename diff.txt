1,7c1
< import logging
< import pathlib
< import re
< import sys
< from dataclasses import dataclass, field
< from typing import Any, Callable, Dict, List, Optional, Set, Union
< import datasets
---
> from datasets import load_dataset, load_metric
9,23d2
< import torch
< from packaging import version
< from torch import nn
< import librosa
< from lang_trans import arabic
< from transformers import (
<     HfArgumentParser,
<     Trainer,
<     TrainingArguments,
<     Wav2Vec2CTCTokenizer,
<     Wav2Vec2FeatureExtractor,
<     Wav2Vec2Processor,
<     is_apex_available,
<     trainer_utils,
< )
24a4,13
> 
> 
> timit = load_dataset("timit_asr", data_dir = "./timit/")
> timit = timit.filter(lambda x: np.random.rand() < 0.1)
> 
> print("dataset:", timit)
> 
> timit = timit.remove_columns(["phonetic_detail", "word_detail", "dialect_region", "id", "sentence_type", "speaker_id"])
> 
> from datasets import ClassLabel
26c15,16
< random.seed(42)
---
> import pandas as pd
> from IPython.display import display, HTML
28,29c18,28
< if is_apex_available():
<     from apex import amp
---
> def show_random_elements(dataset, num_examples=10):
>     assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
>     picks = []
>     for _ in range(num_examples):
>         pick = random.randint(0, len(dataset)-1)
>         while pick in picks:
>             pick = random.randint(0, len(dataset)-1)
>         picks.append(pick)
>     
>     df = pd.DataFrame(dataset[picks])
>     display(HTML(df.to_html()))
31,33c30
< if version.parse(torch.__version__) >= version.parse("1.6"):
<     _is_native_amp_available = True
<     from torch.cuda.amp import autocast
---
> show_random_elements(timit["train"].remove_columns(["audio", "file"]), num_examples=10)
35d31
< logger = logging.getLogger(__name__)
36a33,34
> import re
> chars_to_ignore_regex = '[\,\?\.\!\-\;\:\"]'
38,74c36,38
< @dataclass
< class ModelArguments:
<     """
<     Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
<     """
< 
<     model_name_or_path: str = field(
<         metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
<     )
<     cache_dir: Optional[str] = field(
<         default=None,
<         metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
<     )
<     freeze_feature_extractor: Optional[bool] = field(
<         default=True, metadata={"help": "Whether to freeze the feature extractor layers of the model."}
<     )
<     verbose_logging: Optional[bool] = field(
<         default=False,
<         metadata={"help": "Whether to log verbose messages or not."},
<     )
<     wctc : Optional[bool] = field(
<         default=True, metadata={"help": "Whether to use wctc"},
<     )
< 
< 
< def configure_logger(model_args: ModelArguments, training_args: TrainingArguments):
<     logging.basicConfig(
<         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
<         datefmt="%m/%d/%Y %H:%M:%S",
<         handlers=[logging.StreamHandler(sys.stdout)],
<     )
<     logging_level = logging.WARNING
<     if model_args.verbose_logging:
<         logging_level = logging.DEBUG
<     elif trainer_utils.is_main_process(training_args.local_rank):
<         logging_level = logging.INFO
<     logger.setLevel(logging_level)
---
> def remove_special_characters(batch):
>     batch["text"] = re.sub(chars_to_ignore_regex, '', batch["text"]).lower() + " "
>     return batch
75a40
> timit = timit.map(remove_special_characters)
77,136c42
< @dataclass
< class DataTrainingArguments:
<     """
<     Arguments pertaining to what data we are going to input our model for training and eval.
<     Using `HfArgumentParser` we can turn this class
<     into argparse arguments to be able to specify them on
<     the command line.
<     """
< 
<     dataset_name: str = field(
<         default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
<     )
<     dataset_config_name: Optional[str] = field(
<         default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
<     )
<     train_split_name: Optional[str] = field(
<         default="train",
<         metadata={
<             "help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
<         },
<     )
<     validation_split_name: Optional[str] = field(
<         default="validation",
<         metadata={
<             "help": "The name of the validation data set split to use (via the datasets library). Defaults to 'validation'"
<         },
<     )
<     target_text_column: Optional[str] = field(
<         default="text",
<         metadata={"help": "Column in the dataset that contains label (target text). Defaults to 'text'"},
<     )
<     speech_file_column: Optional[str] = field(
<         default="file",
<         metadata={"help": "Column in the dataset that contains speech file path. Defaults to 'file'"},
<     )
<     target_feature_extractor_sampling_rate: Optional[bool] = field(
<         default=False,
<         metadata={"help": "Resample loaded audio to target feature extractor's sampling rate or not."},
<     )
<     max_duration_in_seconds: Optional[float] = field(
<         default=None,
<         metadata={"help": "Filters out examples longer than specified. Defaults to no filtering."},
<     )
<     orthography: Optional[str] = field(
<         default="librispeech",
<         metadata={
<             "help": "Orthography used for normalization and tokenization: 'librispeech' (default), 'timit', or 'buckwalter'."
<         },
<     )
<     overwrite_cache: bool = field(
<         default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
<     )
<     preprocessing_num_workers: Optional[int] = field(
<         default=None,
<         metadata={"help": "The number of processes to use for the preprocessing."},
<     )
<     mask_ratio: float = field(
<         default=0.0,
<         metadata={"help": "The ratio of label being randomly masked"},
<     )
---
> show_random_elements(timit["train"].remove_columns(["audio", "file"]))
137a44,47
> def extract_all_chars(batch):
>   all_text = " ".join(batch["text"])
>   vocab = list(set(all_text))
>   return {"vocab": [vocab], "all_text": [all_text]}
139,187c49
< @dataclass
< class Orthography:
<     """
<     Orthography scheme used for text normalization and tokenization.
<     Args:
<         do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`False`):
<             Whether or not to accept lowercase input and lowercase the output when decoding.
<         vocab_file (:obj:`str`, `optional`):
<             File containing the vocabulary.
<         word_delimiter_token (:obj:`str`, `optional`, defaults to :obj:`"|"`):
<             The token used for delimiting words; it needs to be in the vocabulary.
<         translation_table (:obj:`Dict[str, str]`, `optional`, defaults to :obj:`{}`):
<             Table to use with `str.translate()` when preprocessing text (e.g., "-" -> " ").
<         words_to_remove (:obj:`Set[str]`, `optional`, defaults to :obj:`set()`):
<             Words to remove when preprocessing text (e.g., "sil").
<         untransliterator (:obj:`Callable[[str], str]`, `optional`):
<             Function that untransliterates text back into native writing system.
<     """
< 
<     do_lower_case: bool = False
<     vocab_file: Optional[str] = None
<     word_delimiter_token: Optional[str] = "|"
<     translation_table: Optional[Dict[str, str]] = field(default_factory=dict)
<     words_to_remove: Optional[Set[str]] = field(default_factory=set)
<     untransliterator: Optional[Callable[[str], str]] = None
< 
<     @classmethod
<     def from_name(cls, name: str):
<         if name == "librispeech":
<             return cls()
<         if name == "timit":
<             return cls(
<                 do_lower_case=True,
<                 # break compounds like "quarter-century-old" and replace pauses "--"
<                 translation_table=str.maketrans({"-": " "}),
<             )
<         if name == "buckwalter":
<             translation_table = {
<                 "-": " ",  # sometimes used to represent pauses
<                 "^": "v",  # fixing "tha" in arabic_speech_corpus dataset
<             }
<             return cls(
<                 vocab_file=pathlib.Path(__file__).parent.joinpath("vocab/buckwalter.json"),
<                 word_delimiter_token="/",  # "|" is Arabic letter alef with madda above
<                 translation_table=str.maketrans(translation_table),
<                 words_to_remove={"sil"},  # fixing "sil" in arabic_speech_corpus dataset
<                 untransliterator=arabic.buckwalter.untransliterate,
<             )
<         raise ValueError(f"Unsupported orthography: '{name}'.")
---
> vocabs = timit.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=timit.column_names["train"])
189,221c51,102
<     def preprocess_for_training(self, text: str) -> str:
<         # TODO(elgeish) return a pipeline (e.g., from jiwer) instead? Or rely on branch predictor as is
<         if len(self.translation_table) > 0:
<             text = text.translate(self.translation_table)
<         if len(self.words_to_remove) == 0:
<             text = " ".join(text.split())  # clean up whitespaces
<         else:
<             text = " ".join(w for w in text.split() if w not in self.words_to_remove)  # and clean up whilespaces
<         return text
< 
<     def create_processor(self, model_args: ModelArguments) -> Wav2Vec2Processor:
<         if model_args.model_name_or_path == 'facebook/wav2vec2-large':
<             model_name = 'facebook/wav2vec2-base'
<         else:
<             model_name = model_args.model_name_or_path
<         feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
<             model_name, cache_dir=model_args.cache_dir
<         )
<         if self.vocab_file:
<             tokenizer = Wav2Vec2CTCTokenizer(
<                 self.vocab_file,
<                 cache_dir=model_args.cache_dir,
<                 do_lower_case=self.do_lower_case,
<                 word_delimiter_token=self.word_delimiter_token,
<             )
<         else:
<             tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(
<                 model_name,
<                 cache_dir=model_args.cache_dir,
<                 do_lower_case=self.do_lower_case,
<                 word_delimiter_token=self.word_delimiter_token,
<             )
<         return Wav2Vec2Processor(feature_extractor, tokenizer)
---
> 
> vocab_list = list(set(vocabs["train"]["vocab"][0]) | set(vocabs["test"]["vocab"][0]))
> 
> vocab_dict = {v: k for k, v in enumerate(vocab_list)}
> vocab_dict
> 
> 
> vocab_dict["|"] = vocab_dict[" "]
> del vocab_dict[" "]
> 
> vocab_dict["[UNK]"] = len(vocab_dict)
> vocab_dict["[PAD]"] = len(vocab_dict)
> len(vocab_dict)
> 
> import json
> with open('vocab.json', 'w') as vocab_file:
>     json.dump(vocab_dict, vocab_file)
> 
> 
> from transformers import Wav2Vec2CTCTokenizer
> 
> tokenizer = Wav2Vec2CTCTokenizer("./vocab.json", unk_token="[UNK]", pad_token="[PAD]", word_delimiter_token="|")
> 
> 
> 
> repo_name = "wav2vec2-base-timit-demo-google-colab"
> 
> 
> from transformers import Wav2Vec2FeatureExtractor
> 
> feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)
> 
> 
> 
> from transformers import Wav2Vec2Processor
> 
> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
> 
> 
> timit["train"][0]["file"]
> 
> 
> timit["train"][0]["audio"]
> 
> import IPython.display as ipd
> import numpy as np
> import random
> 
> rand_int = random.randint(0, len(timit["train"]))
> 
> print(timit["train"][rand_int]["text"])
> ipd.Audio(data=np.asarray(timit["train"][rand_int]["audio"]["array"]), autoplay=True, rate=16000)
223a105,136
> rand_int = random.randint(0, len(timit["train"]))
> 
> print("Target text:", timit["train"][rand_int]["text"])
> print("Input array shape:", np.asarray(timit["train"][rand_int]["audio"]["array"]).shape)
> print("Sampling rate:", timit["train"][rand_int]["audio"]["sampling_rate"])
> 
> 
> def prepare_dataset(batch):
>     audio = batch["audio"]
> 
>     # batched output is "un-batched" to ensure mapping is correct
>     batch["input_values"] = processor(audio["array"], sampling_rate=audio["sampling_rate"]).input_values[0]
>     batch["input_length"] = len(batch["input_values"])
>     
>     with processor.as_target_processor():
>         batch["labels"] = processor(batch["text"]).input_ids
>     return batch
> 
> 
> timit = timit.map(prepare_dataset, remove_columns=timit.column_names["train"], num_proc=4)
> 
> 
> max_input_length_in_sec = 4.0
> timit["train"] = timit["train"].filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=["input_length"])
> 
> 
> 
> import torch
> 
> from dataclasses import dataclass, field
> from typing import Any, Dict, List, Optional, Union
> 
226,248c139,140
<     """
<     Data collator that will dynamically pad the inputs received.
<     Args:
<         processor (:class:`~transformers.Wav2Vec2Processor`)
<             The processor used for proccessing the data.
<         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
<             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
<             among:
<             * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
<               sequence if provided).
<             * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
<               maximum acceptable input length for the model if that argument is not provided.
<             * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
<               different lengths).
<         max_length (:obj:`int`, `optional`):
<             Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
<         max_length_labels (:obj:`int`, `optional`):
<             Maximum length of the ``labels`` returned list and optionally padding length (see above).
<         pad_to_multiple_of (:obj:`int`, `optional`):
<             If set will pad the sequence to a multiple of the provided value.
<             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
<             7.5 (Volta).
<     """
---
> 
> 
251,254d142
<     max_length: Optional[int] = None
<     max_length_labels: Optional[int] = None
<     pad_to_multiple_of: Optional[int] = None
<     pad_to_multiple_of_labels: Optional[int] = None
257c145,146
<         # split inputs and labels since they have to be of different lenghts and need different padding methods
---
>         # split inputs and labels since they have to be of different lenghts and need
>         # different padding methods
264,265d152
<             max_length=self.max_length,
<             pad_to_multiple_of=self.pad_to_multiple_of,
272,273d158
<                 max_length=self.max_length_labels,
<                 pad_to_multiple_of=self.pad_to_multiple_of_labels,
280a166
> 
282a169
> data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
284,406d170
< class CTCTrainer(Trainer):
<     def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
<         """
<         Perform a training step on a batch of inputs.
<         Subclass and override to inject custom behavior.
<         Args:
<             model (:obj:`nn.Module`):
<                 The model to train.
<             inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):
<                 The inputs and targets of the model.
<                 The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
<                 argument :obj:`labels`. Check your model's documentation for all accepted arguments.
<         Return:
<             :obj:`torch.Tensor`: The tensor with training loss on this batch.
<         """
<         model.train()
<         inputs = self._prepare_inputs(inputs)
< 
<         if self.use_amp:
<             with autocast():
<                 loss = self.compute_loss(model, inputs)
<         else:
<             loss = self.compute_loss(model, inputs)
< 
<         if self.args.n_gpu > 1:
<             if model.module.config.ctc_loss_reduction == "mean":
<                 loss = loss.mean()
<             elif model.module.config.ctc_loss_reduction == "sum":
<                 loss = loss.sum() / (inputs["labels"] >= 0).sum()
<             else:
<                 raise ValueError(f"{model.config.ctc_loss_reduction} is not valid. Choose one of ['mean', 'sum']")
< 
<         if self.args.gradient_accumulation_steps > 1:
<             loss = loss / self.args.gradient_accumulation_steps
< 
<         if self.use_amp:
<             self.scaler.scale(loss).backward()
<         elif self.use_apex:
<             with amp.scale_loss(loss, self.optimizer) as scaled_loss:
<                 scaled_loss.backward()
<         elif self.deepspeed:
<             self.deepspeed.backward(loss)
<         else:
<             loss.backward()
< 
<         return loss.detach()
< 
< 
< def main():
<     # See all possible arguments in src/transformers/training_args.py
<     # or by passing the --help flag to this script.
<     # We now keep distinct sets of args, for a cleaner separation of concerns.
< 
<     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
< 
<     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
<     configure_logger(model_args, training_args)
< 
<     orthography = Orthography.from_name(data_args.orthography.lower())
<     processor = orthography.create_processor(model_args)
<     model = Wav2Vec2ForWCTC.from_pretrained(
<         model_args.model_name_or_path,
<         cache_dir=model_args.cache_dir,
<         gradient_checkpointing=training_args.gradient_checkpointing,
<         vocab_size=len(processor.tokenizer),
<         wctc=model_args.wctc,
<     )
< 
<     train_dataset = datasets.load_dataset(
<         data_args.dataset_name, data_args.dataset_config_name, split=data_args.train_split_name
<     )
<     val_dataset = datasets.load_dataset(
<         data_args.dataset_name, data_args.dataset_config_name, split=data_args.validation_split_name
<     )
< 
<     wer_metric = datasets.load_metric("wer")
<     target_sr = processor.feature_extractor.sampling_rate if data_args.target_feature_extractor_sampling_rate else None
<     vocabulary_chars_str = "".join(t for t in processor.tokenizer.get_vocab().keys() if len(t) == 1)
<     vocabulary_text_cleaner = re.compile(  # remove characters not in vocabulary
<         f"[^\s{re.escape(vocabulary_chars_str)}]",  # allow space in addition to chars in vocabulary
<         flags=re.IGNORECASE if processor.tokenizer.do_lower_case else 0,
<     )
<     text_updates = []
< 
<     def prepare_example(example, mask_ratio=0.0):  # TODO(elgeish) make use of multiprocessing?
<         example["speech"], example["sampling_rate"] = librosa.load(example[data_args.speech_file_column], sr=target_sr)
<         if data_args.max_duration_in_seconds is not None:
<             example["duration_in_seconds"] = len(example["speech"]) / example["sampling_rate"]
<         # Normalize and clean up text; order matters!
<         updated_text = orthography.preprocess_for_training(example[data_args.target_text_column])
<         updated_text = vocabulary_text_cleaner.sub("", updated_text)
<         if updated_text != example[data_args.target_text_column]:
<             text_updates.append((example[data_args.target_text_column], updated_text))
<             example[data_args.target_text_column] = updated_text
< 
<         # randomly mask labels based on mask_ratio
<         if mask_ratio > 0:
<             text_len = len(example[data_args.target_text_column])
<             keep_len = round(text_len * (1-mask_ratio))
<             start = random.randint(0, text_len-keep_len)
<             example[data_args.target_text_column] = example[data_args.target_text_column][start:start+keep_len]
< 
<         return example
< 
<     train_dataset = train_dataset.map(prepare_example, fn_kwargs={"mask_ratio": data_args.mask_ratio}, keep_in_memory=True, load_from_cache_file=False, remove_columns=[data_args.speech_file_column])
<     val_dataset = val_dataset.map(prepare_example, remove_columns=[data_args.speech_file_column])
< 
<     if data_args.max_duration_in_seconds is not None:
<         def filter_by_max_duration(example):
<             return example["duration_in_seconds"] <= data_args.max_duration_in_seconds
<         old_train_size = len(train_dataset)
<         old_val_size = len(val_dataset)
<         train_dataset = train_dataset.filter(filter_by_max_duration, remove_columns=["duration_in_seconds"])
<         val_dataset = val_dataset.filter(filter_by_max_duration, remove_columns=["duration_in_seconds"])
<         if len(train_dataset) > old_train_size:
<             logger.warning(
<                 f"Filtered out {len(train_dataset) - old_train_size} train example(s) longer than {data_args.max_duration_in_seconds} second(s)."
<             )
<         if len(val_dataset) > old_val_size:
<             logger.warning(
<                 f"Filtered out {len(val_dataset) - old_val_size} validation example(s) longer than {data_args.max_duration_in_seconds} second(s)."
<             )
<     logger.info(f"Split sizes: {len(train_dataset)} train and {len(val_dataset)} validation.")
408,423c172,243
<     logger.warning(f"Updated {len(text_updates)} transcript(s) using '{data_args.orthography}' orthography rules.")
<     if logger.isEnabledFor(logging.DEBUG):
<         for original_text, updated_text in text_updates:
<             logger.debug(f'Updated text: "{original_text}" -> "{updated_text}"')
<     text_updates = None
< 
<     def prepare_dataset(batch):
<         # check that all files have the correct sampling rate
<         assert (
<             len(set(batch["sampling_rate"])) == 1
<         ), f"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}."
< 
<         batch["input_values"] = processor(batch["speech"], sampling_rate=batch["sampling_rate"][0]).input_values
<         with processor.as_target_processor():
<             batch["labels"] = processor(batch[data_args.target_text_column]).input_ids
<         return batch
---
> wer_metric = load_metric("wer")
> 
> 
> def compute_metrics(pred):
>     pred_logits = pred.predictions
>     pred_ids = np.argmax(pred_logits, axis=-1)
> 
>     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id
> 
>     pred_str = processor.batch_decode(pred_ids)
>     # we do not want to group tokens when computing the metrics
>     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)
> 
>     wer = wer_metric.compute(predictions=pred_str, references=label_str)
> 
>     return {"wer": wer}
> 
> 
> # from transformers import Wav2Vec2ForCTC
> 
> model = Wav2Vec2ForWCTC.from_pretrained(
>     "facebook/wav2vec2-base",
>     ctc_loss_reduction="mean", 
>     pad_token_id=processor.tokenizer.pad_token_id,
> )
> 
> model.freeze_feature_encoder()
> 
> 
> from transformers import TrainingArguments
> 
> training_args = TrainingArguments(
>   output_dir=repo_name,
>   group_by_length=True,
>   per_device_train_batch_size=8,
>   evaluation_strategy="steps",
>   num_train_epochs=1,
>   fp16=True,
>   gradient_checkpointing=True,
>   save_steps=10,
>   eval_steps=1,
>   logging_steps=10,
>   learning_rate=1e-2,
>   weight_decay=0.005,
>   warmup_steps=1000,
>   save_total_limit=2,
> )
> 
> """Now, all instances can be passed to Trainer and we are ready to start training!"""
> 
> from transformers import Trainer
> 
> 
> print("Making trainer:")
> trainer = Trainer(
>     model=model,
>     data_collator=data_collator,
>     args=training_args,
>     compute_metrics=compute_metrics,
>     train_dataset=timit["train"],
>     eval_dataset=timit["test"],
>     tokenizer=processor.feature_extractor,
> )
> 
> 
> print("RUNNING TRAINER.TRAIN()")
> trainer.train()
> 
> 
> print("RUNNING EVAL:")
> eval_results = trainer.evaluate()
> 
425,473c245
<     train_dataset = train_dataset.map(
<         prepare_dataset,
<         batch_size=training_args.per_device_train_batch_size,
<         batched=True,
<         num_proc=data_args.preprocessing_num_workers,
<         keep_in_memory=True,
<         load_from_cache_file=False
<     )
<     val_dataset = val_dataset.map(
<         prepare_dataset,
<         batch_size=training_args.per_device_train_batch_size,
<         batched=True,
<         num_proc=data_args.preprocessing_num_workers,
<     )
< 
<     data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
< 
<     def compute_metrics(pred):
<         pred_logits = pred.predictions
<         pred_ids = np.argmax(pred_logits, axis=-1)
<         pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id
< 
<         pred_str = processor.batch_decode(pred_ids)
<         # we do not want to group tokens when computing the metrics
<         label_str = processor.batch_decode(pred.label_ids, group_tokens=False)
<         if logger.isEnabledFor(logging.DEBUG):
<             for reference, predicted in zip(label_str, pred_str):
<                 logger.debug(f'reference: "{reference}"')
<                 logger.debug(f'predicted: "{predicted}"')
<                 if orthography.untransliterator is not None:
<                     logger.debug(f'reference (untransliterated): "{orthography.untransliterator(reference)}"')
<                     logger.debug(f'predicted (untransliterated): "{orthography.untransliterator(predicted)}"')
< 
<         wer = wer_metric.compute(predictions=pred_str, references=label_str)
<         return {"wer": wer}
< 
<     if model_args.freeze_feature_extractor:
<         model.freeze_feature_extractor()
< 
<     trainer = CTCTrainer(
<         model=model,
<         data_collator=data_collator,
<         args=training_args,
<         compute_metrics=compute_metrics,
<         train_dataset=train_dataset,
<         eval_dataset=val_dataset,
<         tokenizer=processor.feature_extractor,
<     )
<     trainer.train()
---
> print(eval_results)
476,477d247
< if __name__ == "__main__":
<     main()
\ No newline at end of file
